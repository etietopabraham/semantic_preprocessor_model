{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/semantic_preprocessor_model/semantic_preprocessor_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Env Variables in Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export MLFLOW_TRACKING_URI=https://dagshub.com/etietopabraham/semantic_preprocessor_model.mlflow\n",
    "export MLFLOW_TRACKING_USERNAME=etietopabraham\n",
    "export MLFLOW_TRACKING_PASSWORD=324bb2aaa6fc82dbfce509eac2ce2cd6a016a869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow Env Variables in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/etietopabraham/semantic_preprocessor_model.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"etietopabraham\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"324bb2aaa6fc82dbfce509eac2ce2cd6a016a869\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration for Model Evaluation\n",
    "\n",
    "model_evaluation:\n",
    "  # Root directory for saving model evaluation artifacts\n",
    "  root_dir: artifacts/model_evaluation\n",
    "  \n",
    "  # Path to the val data used for evaluation\n",
    "  val_features_path: artifacts/data_transformation/val_features.npz\n",
    "  val_labels_path: artifacts/data_transformation/val_labels.csv\n",
    "  \n",
    "  # Path to the trained model saved during the training step\n",
    "  model_path: artifacts/model_trainer/model.joblib\n",
    "  \n",
    "  # Path to save the evaluation metrics in JSON format\n",
    "  metric_file_path: artifacts/model_evaluation/metrics.json\n",
    "\n",
    "  # MLFlow URI\n",
    "  mlflow_uri: 'https://dagshub.com/etietopabraham/semantic_preprocessor_model.mlflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing configuration related to model evaluation.\n",
    "\n",
    "    Attributes:\n",
    "    - root_dir: Root directory for saving model evaluation artifacts.\n",
    "    - val_features_path: Path to the validation features used for evaluation.\n",
    "    - val_labels_path: Path to the validation labels used for evaluation.\n",
    "    - model_path: Path to the trained model saved during the training step.\n",
    "    - metric_file_name: Name (or path) to save the evaluation metrics.\n",
    "    - mlflow_uri: URI for MLflow tracking server.\n",
    "    - all_params: (Optional) Dictionary containing other relevant parameters.\n",
    "\n",
    "    Note: The `frozen=True` argument makes instances of this class immutable, \n",
    "    ensuring that once an instance is created, its attributes cannot be modified.\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir: Path          # Directory for saving model evaluation artifacts\n",
    "    val_features_path: Path # Path to the validation features\n",
    "    val_labels_path: Path   # Path to the validation labels\n",
    "    model_path: Path        # Path to the saved model\n",
    "    metric_file_path: str   # Filename or path to save evaluation metrics\n",
    "    mlflow_uri: str         # URI for MLflow tracking\n",
    "    all_params: dict        # Other relevant parameters for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model.constants import *\n",
    "from src.semantic_preprocessor_model.utils.common import read_yaml, create_directories\n",
    "from src.semantic_preprocessor_model import logger\n",
    "from src.semantic_preprocessor_model.entity.config_entity import (DataIngestionConfig,\n",
    "                                                                  DataValidationConfig,\n",
    "                                                                  DataTransformationConfig,\n",
    "                                                                  ModelTrainingConfig,\n",
    "                                                                  ModelEvaluationConfig)\n",
    "\n",
    "import os\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Retrieve the configuration related to model evaluation.\n",
    "\n",
    "        This method:\n",
    "        1. Extracts model evaluation configuration from the main configuration.\n",
    "        2. Extracts GradientBoostingRegressor parameters from the params configuration.\n",
    "        3. Retrieves the target column from the feature schema.\n",
    "        4. Ensures the root directory for saving model evaluation artifacts exists.\n",
    "        5. Constructs and returns a ModelEvaluationConfig object.\n",
    "\n",
    "        Returns:\n",
    "            ModelEvaluationConfig: Dataclass object containing configurations for model evaluation.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If an expected attribute does not exist in the config or params files.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            config = self.config.model_evaluation\n",
    "            params = self.params.MLPClassifier\n",
    "\n",
    "            # Ensure the root directory for model evaluation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the ModelEvaluationConfig object\n",
    "            return ModelEvaluationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                val_features_path=Path(config.val_features_path),\n",
    "                val_labels_path=Path(config.val_labels_path),\n",
    "                model_path=config.model_path,\n",
    "                metric_file_path=config.metric_file_path,\n",
    "                all_params=params,\n",
    "                mlflow_uri=config.mlflow_uri,\n",
    "            )\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"An expected attribute does not exist in the config or params files.\")\n",
    "            raise e \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from pathlib import Path\n",
    "from src.semantic_preprocessor_model import logger\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "import mlflow\n",
    "from scipy.sparse import load_npz\n",
    "import ast\n",
    "\n",
    "from src.semantic_preprocessor_model.utils.common import save_json\n",
    "\n",
    "class ModelEvaluation:\n",
    "    \"\"\"\n",
    "    The ModelEvaluation class evaluates the performance of a trained model using \n",
    "    validation data and logs the results into MLflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ConfigurationManager):\n",
    "        \"\"\"\n",
    "        Initialize ModelEvaluation with a configuration manager.\n",
    "\n",
    "        Args:\n",
    "        - config (ConfigurationManager): Configuration manager instance.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.model = None\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for classification.\n",
    "        \n",
    "        Args:\n",
    "        - actual (array-like): True labels.\n",
    "        - pred (array-like): Predicted labels.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Dictionary containing accuracy, precision, recall, and F1 score.\n",
    "        \"\"\"\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        \n",
    "        # Calculate precision, recall, and F1\n",
    "        precision_values, recall_values, f1_values, _ = precision_recall_fscore_support(actual, pred, average='weighted')\n",
    "        \n",
    "        # Take average for logging purposes\n",
    "        precision = precision_values\n",
    "        recall = recall_values\n",
    "        f1 = f1_values\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "        print(f\"Accuracy type: {type(accuracy)}\")\n",
    "        print(f\"All Params: {self.config.all_params}\")\n",
    "        print(f\"Metrics {results}\")\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load validation data and the trained model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load validation data\n",
    "            logger.info(\"Loading validation features...\")\n",
    "            self.X_val = load_npz(self.config.val_features_path)\n",
    "\n",
    "            logger.info(\"Loading validation labels...\")\n",
    "            self.y_val = pd.read_csv(self.config.val_labels_path).iloc[:, 0]\n",
    "\n",
    "            logger.info(\"Loading trained model...\")\n",
    "            self.model = joblib.load(self.config.model_path)\n",
    "\n",
    "            logger.info(\"Data and model loaded successfully.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while loading data or model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"\n",
    "        Log model parameters, metrics, and the model itself into MLflow. This function first loads \n",
    "        the validation data and the trained model. It then predicts on the validation data using \n",
    "        the model and calculates evaluation metrics. These metrics, along with model parameters, \n",
    "        are then logged into MLflow. Finally, the model itself is also logged into MLflow.\n",
    "        \"\"\"\n",
    "        # Logging the start of the MLflow logging process\n",
    "        logger.info(\"Starting MLflow logging...\")\n",
    "\n",
    "        # Load validation data and the trained model\n",
    "        self.load_data()\n",
    "\n",
    "        # Set the MLflow registry URI\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_score = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # If the model configuration contains 'hidden_layer_sizes', convert its string representation to actual tuple\n",
    "        if 'hidden_layer_sizes' in self.config.all_params:\n",
    "            self.config.all_params['hidden_layer_sizes'] = ast.literal_eval(self.config.all_params['hidden_layer_sizes'])\n",
    "\n",
    "        # Start an MLflow tracking session\n",
    "        with mlflow.start_run():\n",
    "            # Predict on validation data\n",
    "            predicted_qualities = self.model.predict(self.X_val)\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            metrics = self.eval_metrics(self.y_val, predicted_qualities)\n",
    "            scores = {\n",
    "                \"accuracy\": metrics['accuracy'], \n",
    "                \"precision\": metrics['precision'], \n",
    "                \"recall\": metrics['recall'], \n",
    "                \"f1\": metrics['f1']\n",
    "            }\n",
    "\n",
    "            # Save the calculated metrics to a JSON file\n",
    "            save_json(path=Path(self.config.metric_file_path), data=scores)\n",
    "\n",
    "            # Log model parameters into MLflow\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "\n",
    "            # Log each metric into MLflow\n",
    "            for key, value in scores.items():\n",
    "                mlflow.log_metric(key, value)\n",
    "\n",
    "            # Determine how to log the model into MLflow based on the tracking URL type\n",
    "            if tracking_url_type_score != \"file\":\n",
    "                mlflow.sklearn.log_model(self.model, \"model\", registered_model_name=\"MLPClassifier\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(self.model, \"model\")\n",
    "\n",
    "        # Logging the completion of the MLflow logging process\n",
    "        logger.info(\"MLflow logging completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-23 06:35:16,469: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-23 06:35:16,470: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-23 06:35:16,472: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-23 06:35:16,473: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-23 06:35:16,473: 13: semantic_preprocessor_model_logger: INFO: 1743914438:  Fetching model evaluation configuration...]\n",
      "[2023-10-23 06:35:16,473: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/model_evaluation]\n",
      "[2023-10-23 06:35:16,474: 16: semantic_preprocessor_model_logger: INFO: 1743914438:  Initializing model evaluation process...]\n",
      "[2023-10-23 06:35:16,474: 19: semantic_preprocessor_model_logger: INFO: 1743914438:  Logging model evaluation into MLFlow...]\n",
      "[2023-10-23 06:35:16,474: 97: semantic_preprocessor_model_logger: INFO: 84091956:  Starting MLflow logging...]\n",
      "[2023-10-23 06:35:16,474: 74: semantic_preprocessor_model_logger: INFO: 84091956:  Loading validation features...]\n",
      "[2023-10-23 06:35:16,493: 77: semantic_preprocessor_model_logger: INFO: 84091956:  Loading validation labels...]\n",
      "[2023-10-23 06:35:16,522: 80: semantic_preprocessor_model_logger: INFO: 84091956:  Loading trained model...]\n",
      "[2023-10-23 06:35:16,539: 83: semantic_preprocessor_model_logger: INFO: 84091956:  Data and model loaded successfully.]\n",
      "Accuracy type: <class 'numpy.float64'>\n",
      "All Params: {'hidden_layer_sizes': (100,), 'max_iter': 500, 'random_state': 42}\n",
      "Metrics {'accuracy': 0.9921510501760119, 'precision': 0.9920478405516039, 'recall': 0.9921510501760119, 'f1': 0.9919377185187034}\n",
      "[2023-10-23 06:35:19,097: 86: semantic_preprocessor_model_logger: INFO: common:  json file saved at: artifacts/model_evaluation/metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/semantic_preprocessor_model_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Registered model 'MLPClassifier' already exists. Creating a new version of this model...\n",
      "2023/10/23 06:36:15 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: MLPClassifier, version 4\n",
      "Created version '4' of model 'MLPClassifier'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-23 06:36:15,903: 141: semantic_preprocessor_model_logger: INFO: 84091956:  MLflow logging completed.]\n",
      "[2023-10-23 06:36:15,905: 22: semantic_preprocessor_model_logger: INFO: 1743914438:  Model Evaluation Pipeline completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    STAGE_NAME = \"Model Evaluation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            logger.info(\"Fetching model evaluation configuration...\")\n",
    "            model_evaluation_configuration = self.config_manager.get_model_evaluation_config()\n",
    "\n",
    "            logger.info(\"Initializing model evaluation process...\")\n",
    "            model_evaluation = ModelEvaluation(config=model_evaluation_configuration)\n",
    "            \n",
    "            logger.info(\"Logging model evaluation into MLFlow...\")\n",
    "            model_evaluation.log_into_mlflow()\n",
    "            \n",
    "            logger.info(\"Model Evaluation Pipeline completed successfully.\")\n",
    "       \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the model evaluation: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = ModelEvaluationPipeline()\n",
    "    pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_preprocessor_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
