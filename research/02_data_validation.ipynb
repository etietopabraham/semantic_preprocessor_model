{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/semantic_preprocessor_model/semantic_preprocessor_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data validation\n",
    "data_validation:\n",
    "  # Directory where data validation results and artifacts are stored\n",
    "  root_dir: artifacts/data_validation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/data.csv\n",
    "  \n",
    "  # Path to the file that captures the validation status (e.g., success, errors encountered)\n",
    "  status_file: artifacts/data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data validation.\n",
    "    \n",
    "    This class captures the essential configurations required for data validation, \n",
    "    including directories for storing validation results, paths to data files, \n",
    "    and the expected data schema.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    root_dir : Path\n",
    "        Directory for storing validation results and related artifacts.\n",
    "        \n",
    "    data_source_file : Path\n",
    "        Path to the ingested or feature-engineered data file.\n",
    "        \n",
    "    status_file : Path\n",
    "        File for logging the validation status.\n",
    "        \n",
    "    schema : Dict[str, Dict[str, str]]\n",
    "        Dictionary containing initial schema configurations for data validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing validation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested or feature-engineered data file\n",
    "    status_file: Path  # File for logging the validation status\n",
    "    schema: Dict[str, Dict[str, str]]  # Dictionary containing initial schema configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "schema_type: \"schema\"\n",
    "description: \"Schema of the data before feature engineering.\"\n",
    "\n",
    "columns:\n",
    "  work_name: \n",
    "    type: object\n",
    "    description: \"Name or identifier of the work task.\"\n",
    "  generalized_work_class: \n",
    "    type: object\n",
    "    description: \"Broad category or class of the work task.\"\n",
    "  global_work_class: \n",
    "    type: object\n",
    "    description: \"Highest level category or class of the work task.\"\n",
    "  upper_works: \n",
    "    type: object\n",
    "    description: \"Higher-level tasks or processes associated with the work task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model.constants import *\n",
    "from src.semantic_preprocessor_model.utils.common import read_yaml, create_directories\n",
    "from src.semantic_preprocessor_model import logger\n",
    "from src.semantic_preprocessor_model.entity.config_entity import DataValidationConfig\n",
    "import os\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Manager for configurations required for the semantic preprocessor data pipeline.\n",
    "\n",
    "    This manager facilitates the reading of configuration, parameter, and schema settings \n",
    "    from specified files. It provides methods to access these settings and ensures the \n",
    "    creation of necessary directories as defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Pipeline parameters.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath=CONFIG_FILE_PATH, \n",
    "                 params_filepath=PARAMS_FILE_PATH, \n",
    "                 schema_filepath=SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration, if they don't exist.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "\n",
    "        # Ensure the directory for storing artifacts exists\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extracts data validation configurations and constructs a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            schema = self.schema.columns\n",
    "            \n",
    "            # Ensure the directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.semantic_preprocessor_model import logger\n",
    "from src.semantic_preprocessor_model.entity.config_entity import DataValidationConfig\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    The DataValidation class ensures the integrity of the dataset by comparing it \n",
    "    against a predefined schema. It verifies the presence and data types of columns \n",
    "    as per the expectations set in the schema.\n",
    "\n",
    "    Attributes:\n",
    "    - df (pd.DataFrame): The data to be validated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define optional and required columns for validation\n",
    "    optional_columns = {'upper_work'}\n",
    "    required_columns = {'work_name', 'generalized_work_class', 'global_work_class'}\n",
    "\n",
    "    def __init__(self, config: DataValidationConfig, file_object=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidation class.\n",
    "        \n",
    "        Depending on the presence of a file_object, it either loads data from the provided \n",
    "        file object or from the specified file in the configuration.\n",
    "\n",
    "        Args:\n",
    "        - config (DataValidationConfig): Configuration settings for data validation.\n",
    "        - file_object (File, optional): A file object containing the dataset.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing DataValidation.\")\n",
    "        self.config = config\n",
    "        try:\n",
    "            if file_object:\n",
    "                self.df = pd.read_csv(file_object)\n",
    "            else:\n",
    "                self.df = pd.read_csv(self.config.data_source_file)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "    def validate_all_features(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if all expected columns, as defined in the schema, are present in the dataframe.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all columns are present and match the schema, False otherwise.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting feature validation.\")\n",
    "        \n",
    "        validation_status = True\n",
    "\n",
    "        all_columns = set(self.df.columns)\n",
    "        expected_columns = set(self.config.schema.keys())\n",
    "\n",
    "        missing_required_columns = self.required_columns - all_columns\n",
    "        extra_columns = all_columns - expected_columns - self.optional_columns\n",
    "\n",
    "        if missing_required_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Missing required columns: {', '.join(missing_required_columns)}\")\n",
    "\n",
    "        if extra_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Extra columns found: {', '.join(extra_columns)}\")\n",
    "\n",
    "        if validation_status:\n",
    "            logger.info(\"All expected columns are present in the dataframe.\")\n",
    "        return validation_status\n",
    "\n",
    "    def validate_data_types(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks the data types of each column in the dataframe against the expected \n",
    "        data types defined in the schema.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all column data types match the schema, False otherwise.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data type validation.\")\n",
    "        validation_status = True\n",
    "        \n",
    "        expected_data_types = {col: self.config.schema[col]['type'] for col in self.config.schema if col in self.df.columns}\n",
    "\n",
    "        for column, dtype in expected_data_types.items():\n",
    "            if not pd.api.types.is_dtype_equal(self.df[column].dtype, dtype):\n",
    "                validation_status = False\n",
    "                logger.warning(f\"Data type mismatch for column '{column}': Expected {dtype} but got {self.df[column].dtype}\")\n",
    "\n",
    "        if validation_status:\n",
    "            logger.info(\"All data types are as expected.\")\n",
    "        return validation_status\n",
    "\n",
    "\n",
    "\n",
    "    def _write_status_to_file(self, message: str, overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        Writes the validation status message to a specified file.\n",
    "\n",
    "        Args:\n",
    "        - message (str): The message to write.\n",
    "        - overwrite (bool, optional): If set to True, overwrites the file. If False, appends to the file.\n",
    "        \"\"\"\n",
    "        logger.info(\"Writing validation status to file.\")\n",
    "        mode = 'w' if overwrite else 'a'\n",
    "        try:\n",
    "            with open(self.config.status_file, mode) as f:\n",
    "                f.write(message + \"\\n\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to status file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_all_validations(self) -> bool:\n",
    "        \"\"\"\n",
    "        Executes all data validations and logs the overall status. \n",
    "        It encompasses both feature existence and data type checks.\n",
    "        \"\"\"\n",
    "        logger.info(\"Running all data validations.\")\n",
    "        feature_validation_status = self.validate_all_features()\n",
    "        data_type_validation_status = self.validate_data_types()\n",
    "\n",
    "        overall_status = \"Overall Validation Status: \"\n",
    "        if feature_validation_status and data_type_validation_status:\n",
    "            overall_status += \"All validations passed.\"\n",
    "            logger.info(overall_status)\n",
    "        else:\n",
    "            overall_status += \"Some validations failed. Check the log for details.\"\n",
    "            logger.error(overall_status)\n",
    "        \n",
    "        self._write_status_to_file(overall_status)\n",
    "        return feature_validation_status and data_type_validation_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-23 21:51:08,049: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-23 21:51:08,052: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-23 21:51:08,054: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-23 21:51:08,055: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-23 21:51:08,055: 53: semantic_preprocessor_model_logger: INFO: 3251083886:  >>>>>> Stage: Data Validation Pipeline started <<<<<<]\n",
      "[2023-10-23 21:51:08,056: 31: semantic_preprocessor_model_logger: INFO: 3251083886:  Fetching initial data validation configuration...]\n",
      "[2023-10-23 21:51:08,056: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/data_validation]\n",
      "[2023-10-23 21:51:08,057: 34: semantic_preprocessor_model_logger: INFO: 3251083886:  Initializing data validation process...]\n",
      "[2023-10-23 21:51:08,057: 31: semantic_preprocessor_model_logger: INFO: 2368032492:  Initializing DataValidation.]\n",
      "[2023-10-23 21:51:08,640: 37: semantic_preprocessor_model_logger: INFO: 3251083886:  Executing Data Validations...]\n",
      "[2023-10-23 21:51:08,641: 117: semantic_preprocessor_model_logger: INFO: 2368032492:  Running all data validations.]\n",
      "[2023-10-23 21:51:08,644: 49: semantic_preprocessor_model_logger: INFO: 2368032492:  Starting feature validation.]\n",
      "[2023-10-23 21:51:08,644: 68: semantic_preprocessor_model_logger: INFO: 2368032492:  All expected columns are present in the dataframe.]\n",
      "[2023-10-23 21:51:08,649: 79: semantic_preprocessor_model_logger: INFO: 2368032492:  Starting data type validation.]\n",
      "[2023-10-23 21:51:08,651: 90: semantic_preprocessor_model_logger: INFO: 2368032492:  All data types are as expected.]\n",
      "[2023-10-23 21:51:08,653: 124: semantic_preprocessor_model_logger: INFO: 2368032492:  Overall Validation Status: All validations passed.]\n",
      "[2023-10-23 21:51:08,657: 103: semantic_preprocessor_model_logger: INFO: 2368032492:  Writing validation status to file.]\n",
      "[2023-10-23 21:51:08,659: 40: semantic_preprocessor_model_logger: INFO: 3251083886:  Initial Data Validation Pipeline completed successfully.]\n",
      "[2023-10-23 21:51:08,669: 55: semantic_preprocessor_model_logger: INFO: 3251083886:  >>>>>> Stage Data Validation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "\n",
    "class InitialDataValidationPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline handles the initial data validation steps.\n",
    "\n",
    "    After the data ingestion stage, it's imperative to ensure the data's integrity\n",
    "    before moving on to feature engineering or model training. This class\n",
    "    orchestrates that validation by checking for correct features and data types.\n",
    "\n",
    "    Attributes:\n",
    "        STAGE_NAME (str): The name of this pipeline stage.\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Data Validation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with a configuration manager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_validation(self):\n",
    "        \"\"\"\n",
    "        Run the set of data validations.\n",
    "        \n",
    "        This method orchestrates the different validation functions to ensure the\n",
    "        dataset's integrity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching initial data validation configuration...\")\n",
    "            data_validation_config = self.config_manager.get_data_validation_config()\n",
    "\n",
    "            logger.info(\"Initializing data validation process...\")\n",
    "            data_validation = DataValidation(config=data_validation_config)\n",
    "\n",
    "            logger.info(\"Executing Data Validations...\")\n",
    "            data_validation.run_all_validations()\n",
    "\n",
    "            logger.info(\"Initial Data Validation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the data validation: {e}\")\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Initial Data Validation Pipeline.\n",
    "        \n",
    "        This method encapsulates the process of the initial data validation and\n",
    "        provides logs for each stage of the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\">>>>>> Stage: {InitialDataValidationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "            self.run_data_validation()\n",
    "            logger.info(f\">>>>>> Stage {InitialDataValidationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the {InitialDataValidationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = InitialDataValidationPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_preprocessor_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
