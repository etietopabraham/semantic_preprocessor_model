{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/semantic_preprocessor_model/semantic_preprocessor_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data transformation\n",
    "data_transformation:\n",
    "  # Directory where data transformation results and artifacts are stored\n",
    "  root_dir: artifacts/data_transformation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/data.csv\n",
    "\n",
    "  # Path to data validation status\n",
    "  data_validation: artifacts/data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the data transformation process.\n",
    "    \n",
    "    This configuration class captures the necessary paths and directories \n",
    "    required for the transformation of data post-ingestion and pre-model training.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data transformation results and artifacts are stored.\n",
    "    - data_source_file: Path to the file where the ingested data is stored that needs to be transformed.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing transformation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested data file for transformation\n",
    "    data_validation: Path # Path to the validated output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model.constants import *\n",
    "from src.semantic_preprocessor_model.utils.common import read_yaml, create_directories\n",
    "from src.semantic_preprocessor_model import logger\n",
    "from src.semantic_preprocessor_model.entity.config_entity import DataValidationConfig, DataTransformationConfig\n",
    "import os\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    The ConfigurationManager manages configuration settings needed throughout the data \n",
    "    pipeline processes, such as data validation and data transformation.\n",
    "\n",
    "    It reads configuration, parameter, and schema settings from specified files and provides \n",
    "    a set of methods to access these settings. Additionally, it ensures that the required \n",
    "    directories specified in the configurations are created.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath=CONFIG_FILE_PATH, \n",
    "                 params_filepath=PARAMS_FILE_PATH, \n",
    "                 schema_filepath=SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration, if they don't exist.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "\n",
    "        # Ensure the directory for storing artifacts exists\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, save_npz\n",
    "import nltk\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    Class responsible for transforming the ingested dataset through various processes:\n",
    "    - Text preprocessing\n",
    "    - Handling missing values\n",
    "    - TF-IDF vectorization of text features\n",
    "    - Data filtering\n",
    "    - Dataset splitting\n",
    "    - Saving the transformed datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation component, loads data, and sets up prerequisites.\n",
    "        \n",
    "        Args:\n",
    "        - config (DataTransformationConfig): Configuration settings for data transformation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.df = self._load_data()\n",
    "        self._download_nltk_resources()\n",
    "        self._initialize_stop_words()\n",
    "        self._handle_missing_values()\n",
    "\n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from the specified source file.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Loaded data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.config.data_source_file)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "    def _download_nltk_resources(self):\n",
    "        \"\"\"Download necessary NLTK resources if they aren't present.\"\"\"\n",
    "        if not nltk.data.find('tokenizers/punkt'):\n",
    "            nltk.download('punkt')\n",
    "        if not nltk.data.find('corpora/stopwords'):\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "    def _initialize_stop_words(self):\n",
    "        \"\"\"Initialize a set of Russian stop words.\"\"\"\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        \"\"\"Handle missing values by replacing NaN in 'upper_works' column with 'Unknown'.\"\"\"\n",
    "        self.df['upper_works'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize, convert to lowercase, and filter out stop words from the text.\n",
    "        \n",
    "        Args:\n",
    "        - text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "        - str: Processed text.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text.lower(), language='russian')\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def apply_text_preprocessing(self):\n",
    "        \"\"\"Apply text preprocessing to 'work_name' and 'upper_works' columns.\"\"\"\n",
    "        self.df['processed_work_name'] = self.df['work_name'].apply(self.preprocess_text)\n",
    "        self.df['processed_upper_works'] = self.df['upper_works'].apply(self.preprocess_text)\n",
    "\n",
    "    def vectorize_text_features(self) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        Vectorize text features using TF-IDF and combine them.\n",
    "\n",
    "        Returns:\n",
    "        - csr_matrix: Combined TF-IDF features.\n",
    "        \"\"\"\n",
    "        vectorizer_work_name = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_work_name = vectorizer_work_name.fit_transform(self.df['processed_work_name'])\n",
    "\n",
    "        vectorizer_upper_works = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_upper_works = vectorizer_upper_works.fit_transform(self.df['processed_upper_works'])\n",
    "\n",
    "        return hstack([tfidf_work_name, tfidf_upper_works]).tocsr()\n",
    "\n",
    "    def filter_data(self, combined_tfidf_features_csr) -> (pd.DataFrame, csr_matrix):\n",
    "        \"\"\"\n",
    "        Filter out singleton classes and rows with missing 'generalized_work_class' values.\n",
    "\n",
    "        Args:\n",
    "        - combined_tfidf_features_csr (csr_matrix): Combined TF-IDF features.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame, csr_matrix: Filtered data and corresponding TF-IDF features.\n",
    "        \"\"\"\n",
    "        trainable_data = self.df[~self.df['generalized_work_class'].isnull()]\n",
    "        class_counts = trainable_data['generalized_work_class'].value_counts()\n",
    "        singleton_classes = class_counts[class_counts == 1]\n",
    "        filtered_data = trainable_data[~trainable_data['generalized_work_class'].isin(singleton_classes.index)]\n",
    "        return filtered_data, combined_tfidf_features_csr[filtered_data.index, :]\n",
    "\n",
    "    def split_data(self, X, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Split data into training and test sets with stratification.\n",
    "\n",
    "        Args:\n",
    "        - X (csr_matrix): Features.\n",
    "        - y (pd.Series): Labels.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Training and test datasets.\n",
    "        \"\"\"\n",
    "        return train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "    def _save_datasets(self, X_train, y_train, X_val, y_val, train_features_filename: str, train_labels_filename: str, val_features_filename: str, val_labels_filename: str):\n",
    "        \"\"\"\n",
    "        Save transformed datasets to specified paths in sparse format.\n",
    "\n",
    "        Args:\n",
    "        - X_train (csr_matrix): Training features.\n",
    "        - y_train (pd.Series): Training labels.\n",
    "        - X_val (csr_matrix): Validation features.\n",
    "        - y_val (pd.Series): Validation labels.\n",
    "        - train_features_filename (str): Name of the file for saving training features.\n",
    "        - train_labels_filename (str): Name of the file for saving training labels.\n",
    "        - val_features_filename (str): Name of the file for saving validation features.\n",
    "        - val_labels_filename (str): Name of the file for saving validation labels.\n",
    "        \"\"\"\n",
    "        train_features_output_path = self.config.root_dir / train_features_filename\n",
    "        train_labels_output_path = self.config.root_dir / train_labels_filename\n",
    "        val_features_output_path = self.config.root_dir / val_features_filename\n",
    "        val_labels_output_path = self.config.root_dir / val_labels_filename\n",
    "        \n",
    "        try:\n",
    "            # Save the training and validation features as csr matrices\n",
    "            save_npz(train_features_output_path, X_train)\n",
    "            save_npz(val_features_output_path, X_val)\n",
    "            logger.info(f\"Training and validation features saved in NPZ format.\")\n",
    "\n",
    "            # Save the training and validation labels as CSV files\n",
    "            y_train.to_csv(train_labels_output_path, index=False)\n",
    "            y_val.to_csv(val_labels_output_path, index=False)\n",
    "            logger.info(f\"Training and validation labels saved in CSV format.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while saving datasets: {e}\")\n",
    "\n",
    "\n",
    "    def transform(self, \n",
    "                train_features_filename: str = \"train_features.npz\", \n",
    "                train_labels_filename: str = \"train_labels.csv\", \n",
    "                val_features_filename: str = \"val_features.npz\", \n",
    "                val_labels_filename: str = \"val_labels.csv\") -> tuple:\n",
    "        \"\"\"\n",
    "        Execute entire data transformation pipeline.\n",
    "\n",
    "        Args:\n",
    "        - train_features_filename (str): Name for saving training features (default: \"train_features.npz\").\n",
    "        - train_labels_filename (str): Name for saving training labels (default: \"train_labels.csv\").\n",
    "        - val_features_filename (str): Name for saving validation features (default: \"val_features.npz\").\n",
    "        - val_labels_filename (str): Name for saving validation labels (default: \"val_labels.csv\").\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Transformed training and validation datasets.\n",
    "        \"\"\"\n",
    "        logger.info(\"Applying text processing\")\n",
    "        self.apply_text_preprocessing()\n",
    "\n",
    "        logger.info(\"Vectorizing text features\")\n",
    "        combined_tfidf_features_csr = self.vectorize_text_features()\n",
    "\n",
    "        logger.info(\"Filtering combined features\")\n",
    "        filtered_data, filtered_features = self.filter_data(combined_tfidf_features_csr)\n",
    "\n",
    "        logger.info(\"Splitting data\")\n",
    "        X_train, X_val, y_train, y_val = self.split_data(filtered_features, filtered_data['generalized_work_class'])\n",
    "\n",
    "        logger.info(\"Saving to artifacts\")\n",
    "        self._save_datasets(X_train, y_train, X_val, y_val, train_features_filename, train_labels_filename, val_features_filename, val_labels_filename)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-23 01:35:03,481: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-23 01:35:03,484: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-23 01:35:03,488: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-23 01:35:03,490: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-23 01:35:03,492: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-23 01:35:03,493: 59: semantic_preprocessor_model_logger: INFO: 2594316775:  Starting the Data Transformation Pipeline.]\n",
      "[2023-10-23 01:35:03,495: 60: semantic_preprocessor_model_logger: INFO: 2594316775:  >>>>>> Stage: Data Transformation Pipeline started <<<<<<]\n",
      "[2023-10-23 01:35:03,496: 27: semantic_preprocessor_model_logger: INFO: 2594316775:  Fetching data transformation configuration...]\n",
      "[2023-10-23 01:35:03,497: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-23 01:35:03,498: 30: semantic_preprocessor_model_logger: INFO: 2594316775:  Initializing data transformation...]\n",
      "[2023-10-23 01:35:04,339: 33: semantic_preprocessor_model_logger: INFO: 2594316775:  Executing data transformation pipeline...]\n",
      "[2023-10-23 01:35:04,339: 179: semantic_preprocessor_model_logger: INFO: 3342965432:  Applying text processing]\n",
      "[2023-10-23 01:36:09,415: 182: semantic_preprocessor_model_logger: INFO: 3342965432:  Vectorizing text features]\n",
      "[2023-10-23 01:36:12,922: 185: semantic_preprocessor_model_logger: INFO: 3342965432:  Filtering combined features]\n",
      "[2023-10-23 01:36:13,045: 188: semantic_preprocessor_model_logger: INFO: 3342965432:  Splitting data]\n",
      "[2023-10-23 01:36:13,386: 191: semantic_preprocessor_model_logger: INFO: 3342965432:  Saving to artifacts]\n",
      "[2023-10-23 01:36:13,921: 151: semantic_preprocessor_model_logger: INFO: 3342965432:  Training and validation features saved in NPZ format.]\n",
      "[2023-10-23 01:36:14,290: 156: semantic_preprocessor_model_logger: INFO: 3342965432:  Training and validation labels saved in CSV format.]\n",
      "[2023-10-23 01:36:14,298: 36: semantic_preprocessor_model_logger: INFO: 2594316775:  Shape of X_train: (237484, 5015)]\n",
      "[2023-10-23 01:36:14,298: 37: semantic_preprocessor_model_logger: INFO: 2594316775:  Shape of X_val: (59371, 5015)]\n",
      "[2023-10-23 01:36:14,299: 38: semantic_preprocessor_model_logger: INFO: 2594316775:  Shape of y_train: (237484,)]\n",
      "[2023-10-23 01:36:14,299: 39: semantic_preprocessor_model_logger: INFO: 2594316775:  Shape of y_val: (59371,)]\n",
      "[2023-10-23 01:36:14,299: 41: semantic_preprocessor_model_logger: INFO: 2594316775:  Data Transformation Pipeline completed successfully.]\n",
      "[2023-10-23 01:36:14,330: 62: semantic_preprocessor_model_logger: INFO: 2594316775:  >>>>>> Stage: Data Transformation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates data transformation processes:\n",
    "    - Text preprocessing\n",
    "    - Missing value handling\n",
    "    - Text feature vectorization\n",
    "    - Data filtering and splitting\n",
    "    - Saving transformed datasets\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Data Transformation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pipeline with a configuration manager.\"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_transformation(self):\n",
    "        \"\"\"\n",
    "        Execute data transformation steps and log each stage.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the data transformation process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching data transformation configuration...\")\n",
    "            data_transformation_config = self.config_manager.get_data_transformation_config()\n",
    "\n",
    "            logger.info(\"Initializing data transformation...\")\n",
    "            data_transformer = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "            logger.info(\"Executing data transformation pipeline...\")\n",
    "            X_train, X_val, y_train, y_val = data_transformer.transform()\n",
    "            \n",
    "            logger.info(f\"Shape of X_train: {X_train.shape}\")\n",
    "            logger.info(f\"Shape of X_val: {X_val.shape}\")\n",
    "            logger.info(f\"Shape of y_train: {y_train.shape}\")\n",
    "            logger.info(f\"Shape of y_val: {y_val.shape}\")\n",
    "\n",
    "            logger.info(\"Data Transformation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data transformation: {e}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Data Transformation Pipeline, checking data validations before proceeding.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the pipeline execution.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.config_manager.get_data_transformation_config().data_validation, \"r\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Ensure the validations have passed before running the pipeline\n",
    "            if \"Overall Validation Status: All validations passed.\" in content:\n",
    "                logger.info(\"Starting the Data Transformation Pipeline.\")\n",
    "                logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "                self.run_data_transformation()\n",
    "                logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "            else:\n",
    "                logger.error(\"Pipeline aborted due to validation errors.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during {DataTransformationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataTransformationPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_preprocessor_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
