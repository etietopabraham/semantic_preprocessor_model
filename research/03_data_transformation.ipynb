{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/semantic_preprocessor_model/semantic_preprocessor_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data transformation\n",
    "data_transformation:\n",
    "  # Directory where data transformation results and artifacts are stored\n",
    "  root_dir: artifacts/data_transformation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/data.csv\n",
    "\n",
    "  # Path to data validation status\n",
    "  data_validation: artifacts/data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the data transformation process.\n",
    "    \n",
    "    This configuration class captures the necessary paths and directories \n",
    "    required for the transformation of data post-ingestion and pre-model training.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data transformation results and artifacts are stored.\n",
    "    - data_source_file: Path to the file where the ingested data is stored that needs to be transformed.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing transformation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested data file for transformation\n",
    "    data_validation: Path # Path to the validated output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model.constants import *\n",
    "from src.semantic_preprocessor_model.utils.common import read_yaml, create_directories\n",
    "from src.semantic_preprocessor_model import logger\n",
    "from src.semantic_preprocessor_model.entity.config_entity import DataValidationConfig, DataTransformationConfig\n",
    "import os\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    The ConfigurationManager manages configuration settings needed throughout the data \n",
    "    pipeline processes, such as data validation and data transformation.\n",
    "\n",
    "    It reads configuration, parameter, and schema settings from specified files and provides \n",
    "    a set of methods to access these settings. Additionally, it ensures that the required \n",
    "    directories specified in the configurations are created.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath=CONFIG_FILE_PATH, \n",
    "                 params_filepath=PARAMS_FILE_PATH, \n",
    "                 schema_filepath=SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration, if they don't exist.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "\n",
    "        # Ensure the directory for storing artifacts exists\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, save_npz\n",
    "import nltk\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    Class responsible for transforming the ingested dataset through various processes:\n",
    "    - Text preprocessing\n",
    "    - Handling missing values\n",
    "    - TF-IDF vectorization of text features\n",
    "    - Data filtering\n",
    "    - Dataset splitting\n",
    "    - Saving the transformed datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation component, loads data, and sets up prerequisites.\n",
    "        \n",
    "        Args:\n",
    "        - config (DataTransformationConfig): Configuration settings for data transformation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.df = self._load_data()\n",
    "        self._download_nltk_resources()\n",
    "        self._initialize_stop_words()\n",
    "        self._handle_missing_values()\n",
    "\n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from the specified source file.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Loaded data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.config.data_source_file)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "    def _download_nltk_resources(self):\n",
    "        \"\"\"Download necessary NLTK resources if they aren't present.\"\"\"\n",
    "        if not nltk.data.find('tokenizers/punkt'):\n",
    "            nltk.download('punkt')\n",
    "        if not nltk.data.find('corpora/stopwords'):\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "    def _initialize_stop_words(self):\n",
    "        \"\"\"Initialize a set of Russian stop words.\"\"\"\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        \"\"\"Handle missing values by replacing NaN in 'upper_works' column with 'Unknown'.\"\"\"\n",
    "        self.df['upper_works'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize, convert to lowercase, and filter out stop words from the text.\n",
    "        \n",
    "        Args:\n",
    "        - text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "        - str: Processed text.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text.lower(), language='russian')\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def apply_text_preprocessing(self):\n",
    "        \"\"\"Apply text preprocessing to 'work_name' and 'upper_works' columns.\"\"\"\n",
    "        self.df['processed_work_name'] = self.df['work_name'].apply(self.preprocess_text)\n",
    "        self.df['processed_upper_works'] = self.df['upper_works'].apply(self.preprocess_text)\n",
    "\n",
    "    def vectorize_text_features(self) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        Vectorize text features using TF-IDF and combine them.\n",
    "\n",
    "        Returns:\n",
    "        - csr_matrix: Combined TF-IDF features.\n",
    "        \"\"\"\n",
    "        vectorizer_work_name = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_work_name = vectorizer_work_name.fit_transform(self.df['processed_work_name'])\n",
    "\n",
    "        vectorizer_upper_works = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_upper_works = vectorizer_upper_works.fit_transform(self.df['processed_upper_works'])\n",
    "\n",
    "        return hstack([tfidf_work_name, tfidf_upper_works]).tocsr()\n",
    "\n",
    "    def filter_data(self, combined_tfidf_features_csr) -> (pd.DataFrame, csr_matrix):\n",
    "        \"\"\"\n",
    "        Filter out singleton classes and rows with missing 'generalized_work_class' values.\n",
    "\n",
    "        Args:\n",
    "        - combined_tfidf_features_csr (csr_matrix): Combined TF-IDF features.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame, csr_matrix: Filtered data and corresponding TF-IDF features.\n",
    "        \"\"\"\n",
    "        trainable_data = self.df[~self.df['generalized_work_class'].isnull()]\n",
    "        class_counts = trainable_data['generalized_work_class'].value_counts()\n",
    "        singleton_classes = class_counts[class_counts == 1]\n",
    "        filtered_data = trainable_data[~trainable_data['generalized_work_class'].isin(singleton_classes.index)]\n",
    "        return filtered_data, combined_tfidf_features_csr[filtered_data.index, :]\n",
    "\n",
    "    def split_data(self, X, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Split data into training and test sets with stratification.\n",
    "\n",
    "        Args:\n",
    "        - X (csr_matrix): Features.\n",
    "        - y (pd.Series): Labels.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Training and test datasets.\n",
    "        \"\"\"\n",
    "        return train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "    def _save_datasets(self, X_train, y_train, X_test, y_test, train_features_filename: str, train_labels_filename: str, test_features_filename: str, test_labels_filename: str):\n",
    "        \"\"\"\n",
    "        Save transformed datasets to specified paths in sparse format.\n",
    "\n",
    "        Args:\n",
    "        - X_train (csr_matrix): Training features.\n",
    "        - y_train (pd.Series): Training labels.\n",
    "        - X_test (csr_matrix): Test features.\n",
    "        - y_test (pd.Series): Test labels.\n",
    "        - train_features_filename (str): Name of the file for saving training features.\n",
    "        - train_labels_filename (str): Name of the file for saving training labels.\n",
    "        - test_features_filename (str): Name of the file for saving test features.\n",
    "        - test_labels_filename (str): Name of the file for saving test labels.\n",
    "        \"\"\"\n",
    "        train_features_output_path = self.config.root_dir / train_features_filename\n",
    "        train_labels_output_path = self.config.root_dir / train_labels_filename\n",
    "        test_features_output_path = self.config.root_dir / test_features_filename\n",
    "        test_labels_output_path = self.config.root_dir / test_labels_filename\n",
    "        \n",
    "        try:\n",
    "            # Save the training and test features as csr matrices\n",
    "            save_npz(train_features_output_path, X_train)\n",
    "            save_npz(test_features_output_path, X_test)\n",
    "            logger.info(f\"Training and test features saved in NPZ format.\")\n",
    "\n",
    "            # Save the training and test labels as CSV files\n",
    "            y_train.to_csv(train_labels_output_path, index=False)\n",
    "            y_test.to_csv(test_labels_output_path, index=False)\n",
    "            logger.info(f\"Training and test labels saved in CSV format.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while saving datasets: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, train_features_filename: str = \"train_features.npz\", \n",
    "                train_labels_filename: str = \"train_labels.csv\", \n",
    "                test_features_filename: str = \"test_features.npz\", \n",
    "                test_labels_filename: str = \"test_labels.csv\") -> tuple:\n",
    "        \"\"\"\n",
    "        Execute entire data transformation pipeline.\n",
    "\n",
    "        Args:\n",
    "        - train_features_filename (str): Name for saving training features (default: \"train_features.npz\").\n",
    "        - train_labels_filename (str): Name for saving training labels (default: \"train_labels.csv\").\n",
    "        - test_features_filename (str): Name for saving test features (default: \"test_features.npz\").\n",
    "        - test_labels_filename (str): Name for saving test labels (default: \"test_labels.csv\").\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Transformed training and test datasets.\n",
    "        \"\"\"\n",
    "        logger.info(\"Applying text processing\")\n",
    "        self.apply_text_preprocessing()\n",
    "\n",
    "        logger.info(\"Vectoriing text features\")\n",
    "        combined_tfidf_features_csr = self.vectorize_text_features()\n",
    "\n",
    "        logger.info(\"Filtering combined features\")\n",
    "        filtered_data, filtered_features = self.filter_data(combined_tfidf_features_csr)\n",
    "\n",
    "        logger.info(\"Spliting data\")\n",
    "        X_train, X_test, y_train, y_test = self.split_data(filtered_features, filtered_data['generalized_work_class'])\n",
    "\n",
    "        logger.info(\"saving to artifacts\")\n",
    "        self._save_datasets(X_train, y_train, X_test, y_test, train_features_filename, train_labels_filename, test_features_filename, test_labels_filename)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-22 22:30:07,071: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-22 22:30:07,072: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-22 22:30:07,075: 42: semantic_preprocessor_model_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-22 22:30:07,075: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-22 22:30:07,076: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-22 22:30:07,077: 48: semantic_preprocessor_model_logger: INFO: 788778900:  Starting the Data Transformation Pipeline.]\n",
      "[2023-10-22 22:30:07,077: 49: semantic_preprocessor_model_logger: INFO: 788778900:  >>>>>> Stage: Data Transformation Pipeline started <<<<<<]\n",
      "[2023-10-22 22:30:07,078: 22: semantic_preprocessor_model_logger: INFO: 788778900:  Fetching data transformation configuration...]\n",
      "[2023-10-22 22:30:07,078: 65: semantic_preprocessor_model_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-22 22:30:07,079: 25: semantic_preprocessor_model_logger: INFO: 788778900:  Initializing data transformation...]\n",
      "[2023-10-22 22:30:07,892: 28: semantic_preprocessor_model_logger: INFO: 788778900:  Executing data transformation pipeline...]\n",
      "[2023-10-22 22:30:07,892: 180: semantic_preprocessor_model_logger: INFO: 1769729103:  Applying text processing]\n",
      "[2023-10-22 22:31:11,361: 183: semantic_preprocessor_model_logger: INFO: 1769729103:  Vectoriing text features]\n",
      "[2023-10-22 22:31:14,988: 186: semantic_preprocessor_model_logger: INFO: 1769729103:  Filtering combined features]\n",
      "[2023-10-22 22:31:15,138: 189: semantic_preprocessor_model_logger: INFO: 1769729103:  Spliting data]\n",
      "[2023-10-22 22:31:15,493: 192: semantic_preprocessor_model_logger: INFO: 1769729103:  saving to artifacts]\n",
      "[2023-10-22 22:31:16,037: 152: semantic_preprocessor_model_logger: INFO: 1769729103:  Training and test features saved in NPZ format.]\n",
      "[2023-10-22 22:31:16,368: 157: semantic_preprocessor_model_logger: INFO: 1769729103:  Training and test labels saved in CSV format.]\n",
      "[2023-10-22 22:31:16,379: 31: semantic_preprocessor_model_logger: INFO: 788778900:  Shape of X_train: (237484, 5015)]\n",
      "[2023-10-22 22:31:16,379: 32: semantic_preprocessor_model_logger: INFO: 788778900:  Shape of X_test: (59371, 5015)]\n",
      "[2023-10-22 22:31:16,380: 33: semantic_preprocessor_model_logger: INFO: 788778900:  Shape of y_train: (237484,)]\n",
      "[2023-10-22 22:31:16,380: 34: semantic_preprocessor_model_logger: INFO: 788778900:  Shape of y_test: (59371,)]\n",
      "[2023-10-22 22:31:16,380: 36: semantic_preprocessor_model_logger: INFO: 788778900:  Data Transformation Pipeline completed successfully.]\n",
      "[2023-10-22 22:31:16,415: 51: semantic_preprocessor_model_logger: INFO: 788778900:  >>>>>> Stage: Data Transformation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_preprocessor_model import logger\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates data transformation processes:\n",
    "    - Text preprocessing\n",
    "    - Missing value handling\n",
    "    - Text feature vectorization\n",
    "    - Data filtering and splitting\n",
    "    - Saving transformed datasets\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Data Transformation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pipeline with a configuration manager.\"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_transformation(self):\n",
    "        \"\"\"Execute data transformation steps and log each stage.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching data transformation configuration...\")\n",
    "            data_transformation_config = self.config_manager.get_data_transformation_config()\n",
    "\n",
    "            logger.info(\"Initializing data transformation...\")\n",
    "            data_transformer = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "            logger.info(\"Executing data transformation pipeline...\")\n",
    "            X_train, X_test, y_train, y_test = data_transformer.transform()\n",
    "            \n",
    "            logger.info(f\"Shape of X_train: {X_train.shape}\")\n",
    "            logger.info(f\"Shape of X_test: {X_test.shape}\")\n",
    "            logger.info(f\"Shape of y_train: {y_train.shape}\")\n",
    "            logger.info(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "            logger.info(\"Data Transformation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data transformation: {e}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the entire Data Transformation Pipeline, checking data validations before proceeding.\"\"\"\n",
    "        try:\n",
    "            with open(self.config_manager.get_data_transformation_config().data_validation, \"r\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            if \"Overall Validation Status: All validations passed.\" in content:\n",
    "                logger.info(\"Starting the Data Transformation Pipeline.\")\n",
    "                logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "                self.run_data_transformation()\n",
    "                logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "            else:\n",
    "                logger.error(\"Pipeline aborted due to validation errors.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during {DataTransformationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataTransformationPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_preprocessor_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
